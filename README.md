# ğŸš¢ Titanic Classification

This project predicts whether a passenger survived the Titanic disaster using machine learning.  
It demonstrates data preprocessing, exploratory data analysis (EDA), and model building with Python.

## ğŸ“Š Project Overview

- **Goal:** Predict passenger survival based on features such as age, sex, ticket class, and fare.  
- **Dataset:** Titanic dataset from [Kaggle](https://www.kaggle.com/c/titanic).  
- **Tech Stack:** Python, Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn.  
- **Models Used:** Logistic Regression, Random Forest, and XGBoost.

## âš™ï¸ Steps

1. Data cleaning and preprocessing  
2. Exploratory Data Analysis (EDA)  
3. Feature engineering  
4. Model training and evaluation  
5. Prediction on test data

## ğŸš€ Results

Achieved around **80% accuracy** on the test set.

## ğŸ“ Project Structure
```
titanic-classification/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ test.csv
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train_processed.csv
â”‚       â””â”€â”€ test_processed.csv
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â””â”€â”€ 03_modeling.ipynb
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â””â”€â”€ model.py
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ logistic_regression.pkl
â”‚   â”œâ”€â”€ random_forest.pkl
â”‚   â””â”€â”€ xgboost.pkl
â”œâ”€â”€ ğŸ“„ requirements.txt
â””â”€â”€ ğŸ“„ README.md
```

## ğŸ› ï¸ Installation
```bash
# Clone the repository
git clone https://github.com/yourusername/titanic-classification.git
cd titanic-classification

# Install required packages
pip install -r requirements.txt
```

## ğŸš€ Usage
```python
# Run the main script
python src/model.py

# Or explore the notebooks
jupyter notebook notebooks/01_eda.ipynb
```

## ğŸ“ˆ Model Performance

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| Logistic Regression | 78% | 0.76 | 0.72 | 0.74 |
| Random Forest | 80% | 0.79 | 0.77 | 0.78 |
| XGBoost | 81% | 0.80 | 0.78 | 0.79 |

## ğŸ“ Key Features

- Age imputation using median values
- Feature engineering (family size, title extraction)
- Handling missing values in Cabin and Embarked
- One-hot encoding for categorical variables
- Feature scaling for numerical variables

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- Kaggle for providing the dataset
- The data science community for inspiration and resources
